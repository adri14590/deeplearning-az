{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "rpVgSST_Y455",
        "outputId": "aa3258c9-eacb-4099-a711-bba9fd4b507d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in c:\\users\\aag\\anaconda3\\lib\\site-packages (2.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kj7yYesKbctW"
      },
      "source": [
        "# Parte 1 - Construir el modelo de CNN\n",
        "\n",
        "# Importar las librerías y paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YBacLjT1Y81H"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "# Existen las capas 3D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtczsRskbj2F"
      },
      "source": [
        "# Inicializar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rQL9sTJ9Y_Vc"
      },
      "outputs": [],
      "source": [
        "classifier = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tUbAgrrqbpsY"
      },
      "source": [
        "# Paso 1 - Convolución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2kYdJsjsZCR8"
      },
      "outputs": [],
      "source": [
        "# - Filters: especifica el número de filtros, es decir la dimensionalidad del espacio de salida. Se suelen usar potencias de 2 ya que están más optimizadas.\n",
        "#            Se suele empezar por pocos filtros y luego ir mejorando.\n",
        "# - Kernel_size especifica el tamaño de la matriz de filtros.\n",
        "# - input_shape: tamaño esperado de las imágenes de entrada. Al ser imágenes de color tenemos tres dimensiones. Una matriz de 64x64 de cada color.\n",
        "#                Interesa empezar con imágenes de poco tamaño y luego ir subiendo. Hay que pensar si hay que mantener tres canales o no.\n",
        "# - activation: función para eliminar la linealidad del problema.\n",
        "classifier.add(Conv2D(filters = 32, kernel_size = (3, 3), \n",
        "                      input_shape = (128, 128, 3), activation = \"relu\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rBFya76KbsKw"
      },
      "source": [
        "# Paso 2 - Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jty3bSFYZD98"
      },
      "outputs": [],
      "source": [
        "# Cuanto más grande sea la matriz de pooling más información perderemos aunque ganaremos más rendimiento.\n",
        "# Por defecto el stride es el tamaño de pool_size, es decir se va moviendo la ventana al completo y no hay superposición.\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IgJ4s9fSb8bm"
      },
      "source": [
        "# Una segunda capa de convolución y max pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mkVuO25YZHtg"
      },
      "outputs": [],
      "source": [
        "# Para mejorar el modelo podemos:\n",
        "# 1. Añadir más capas de convolución y pooling. Una práctica común es tener tres capas de convolución y pooling: 32 en primera capa, 32 en segunda capa y más filtros en la tercera, por ejemplo 64.\n",
        "# 2. Añadir más nodos a la capa totalmente conectada.\n",
        "# Obtamos por la 1.\n",
        "classifier.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = \"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Una tercera capa\n",
        "\n",
        "Añadimos una tercera capa con el objetivo de mejorar la precisión de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = \"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfacNAkPb_T-"
      },
      "source": [
        "# Paso 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JWl9_DHxZJZD"
      },
      "outputs": [],
      "source": [
        "classifier.add(Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ksz0Q4WGcB77"
      },
      "source": [
        "# Paso 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KQnTQSjyZKXs"
      },
      "outputs": [],
      "source": [
        "# Estamos eligiendo 128 nodos en la capa de salida de la capa totalmente conectada. \n",
        "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
        "\n",
        "# Probamos a usar dropout\n",
        "classifier.add(Dropout(rate = 0.5))\n",
        "\n",
        "# Estamos utilizando 1 unidad de salida porque nuestro problema de clasificación es binario, es decir, la probabilidad de ser gato es la complementaria a ser perro o viceversa.\n",
        "# Si se tienen más categorías hay que tener más neuronas de salida y utilizar softmax.\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p7INHvHmcFdL"
      },
      "source": [
        "# Compilar la CNN\n",
        "# Como va a ser entrenada?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Uc7pBop7ZLeN"
      },
      "outputs": [],
      "source": [
        "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hits0FnccMjr"
      },
      "source": [
        "# Parte 2 - Ajustar la CNN a las imágenes para entrenar "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "colab_type": "code",
        "id": "BYgCwVDFZMrU",
        "outputId": "641d711f-0723-447f-811b-1c1aaa639274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/90\n",
            "250/250 [==============================] - 98s 390ms/step - loss: 0.6677 - accuracy: 0.5817 - val_loss: 0.6903 - val_accuracy: 0.5625\n",
            "Epoch 2/90\n",
            "250/250 [==============================] - 103s 410ms/step - loss: 0.6003 - accuracy: 0.6799 - val_loss: 0.5523 - val_accuracy: 0.7334\n",
            "Epoch 3/90\n",
            "250/250 [==============================] - 92s 367ms/step - loss: 0.5638 - accuracy: 0.7085 - val_loss: 0.5131 - val_accuracy: 0.7510\n",
            "Epoch 4/90\n",
            "250/250 [==============================] - 95s 380ms/step - loss: 0.5325 - accuracy: 0.7399 - val_loss: 0.5060 - val_accuracy: 0.7535\n",
            "Epoch 5/90\n",
            "250/250 [==============================] - 94s 375ms/step - loss: 0.5022 - accuracy: 0.7584 - val_loss: 0.5038 - val_accuracy: 0.7566\n",
            "Epoch 6/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.4806 - accuracy: 0.7661 - val_loss: 0.4953 - val_accuracy: 0.7692\n",
            "Epoch 7/90\n",
            "250/250 [==============================] - 92s 367ms/step - loss: 0.4461 - accuracy: 0.7930 - val_loss: 0.4491 - val_accuracy: 0.8044\n",
            "Epoch 8/90\n",
            "250/250 [==============================] - 93s 370ms/step - loss: 0.4312 - accuracy: 0.8025 - val_loss: 0.4376 - val_accuracy: 0.7944\n",
            "Epoch 9/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.4116 - accuracy: 0.8154 - val_loss: 0.4310 - val_accuracy: 0.8024\n",
            "Epoch 10/90\n",
            "250/250 [==============================] - 92s 370ms/step - loss: 0.4013 - accuracy: 0.8151 - val_loss: 0.4144 - val_accuracy: 0.8206\n",
            "Epoch 11/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.3907 - accuracy: 0.8246 - val_loss: 0.4218 - val_accuracy: 0.8115\n",
            "Epoch 12/90\n",
            "250/250 [==============================] - 93s 370ms/step - loss: 0.3721 - accuracy: 0.8284 - val_loss: 0.4073 - val_accuracy: 0.8231\n",
            "Epoch 13/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.3563 - accuracy: 0.8376 - val_loss: 0.3987 - val_accuracy: 0.8367\n",
            "Epoch 14/90\n",
            "250/250 [==============================] - 92s 366ms/step - loss: 0.3469 - accuracy: 0.8482 - val_loss: 0.4398 - val_accuracy: 0.8120\n",
            "Epoch 15/90\n",
            "250/250 [==============================] - 92s 367ms/step - loss: 0.3407 - accuracy: 0.8531 - val_loss: 0.4177 - val_accuracy: 0.8266\n",
            "Epoch 16/90\n",
            "250/250 [==============================] - 92s 370ms/step - loss: 0.3290 - accuracy: 0.8547 - val_loss: 0.4339 - val_accuracy: 0.8322\n",
            "Epoch 17/90\n",
            "250/250 [==============================] - 92s 367ms/step - loss: 0.3145 - accuracy: 0.8611 - val_loss: 0.4078 - val_accuracy: 0.8246\n",
            "Epoch 18/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.3110 - accuracy: 0.8669 - val_loss: 0.4427 - val_accuracy: 0.8291\n",
            "Epoch 19/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.2963 - accuracy: 0.8739 - val_loss: 0.4187 - val_accuracy: 0.8332\n",
            "Epoch 20/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.2928 - accuracy: 0.8756 - val_loss: 0.3903 - val_accuracy: 0.8468\n",
            "Epoch 21/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.2867 - accuracy: 0.8813 - val_loss: 0.3758 - val_accuracy: 0.8564\n",
            "Epoch 22/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.2758 - accuracy: 0.8830 - val_loss: 0.4020 - val_accuracy: 0.8448\n",
            "Epoch 23/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.2638 - accuracy: 0.8867 - val_loss: 0.3934 - val_accuracy: 0.8518\n",
            "Epoch 24/90\n",
            "250/250 [==============================] - 93s 371ms/step - loss: 0.2634 - accuracy: 0.8860 - val_loss: 0.3988 - val_accuracy: 0.8503\n",
            "Epoch 25/90\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.2522 - accuracy: 0.8986 - val_loss: 0.4111 - val_accuracy: 0.8528\n",
            "Epoch 26/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.2377 - accuracy: 0.8984 - val_loss: 0.4348 - val_accuracy: 0.8508\n",
            "Epoch 27/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.2331 - accuracy: 0.9028 - val_loss: 0.4266 - val_accuracy: 0.8533\n",
            "Epoch 28/90\n",
            "250/250 [==============================] - 93s 370ms/step - loss: 0.2379 - accuracy: 0.9010 - val_loss: 0.4497 - val_accuracy: 0.8357\n",
            "Epoch 29/90\n",
            "250/250 [==============================] - 96s 384ms/step - loss: 0.2160 - accuracy: 0.9143 - val_loss: 0.4411 - val_accuracy: 0.8599\n",
            "Epoch 30/90\n",
            "250/250 [==============================] - 91s 364ms/step - loss: 0.2100 - accuracy: 0.9107 - val_loss: 0.4159 - val_accuracy: 0.8564\n",
            "Epoch 31/90\n",
            "250/250 [==============================] - 82s 328ms/step - loss: 0.2172 - accuracy: 0.9121 - val_loss: 0.4054 - val_accuracy: 0.8558\n",
            "Epoch 32/90\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.2114 - accuracy: 0.9121 - val_loss: 0.4325 - val_accuracy: 0.8523\n",
            "Epoch 33/90\n",
            "250/250 [==============================] - 81s 323ms/step - loss: 0.2001 - accuracy: 0.9172 - val_loss: 0.4547 - val_accuracy: 0.8533\n",
            "Epoch 34/90\n",
            "250/250 [==============================] - 81s 325ms/step - loss: 0.1979 - accuracy: 0.9199 - val_loss: 0.4402 - val_accuracy: 0.8528\n",
            "Epoch 35/90\n",
            "250/250 [==============================] - 94s 375ms/step - loss: 0.1889 - accuracy: 0.9235 - val_loss: 0.3916 - val_accuracy: 0.8740\n",
            "Epoch 36/90\n",
            "250/250 [==============================] - 96s 385ms/step - loss: 0.1870 - accuracy: 0.9265 - val_loss: 0.4456 - val_accuracy: 0.8579\n",
            "Epoch 37/90\n",
            "250/250 [==============================] - 99s 397ms/step - loss: 0.1909 - accuracy: 0.9205 - val_loss: 0.4560 - val_accuracy: 0.8634\n",
            "Epoch 38/90\n",
            "250/250 [==============================] - 112s 447ms/step - loss: 0.1894 - accuracy: 0.9237 - val_loss: 0.4299 - val_accuracy: 0.8553\n",
            "Epoch 39/90\n",
            "250/250 [==============================] - 112s 447ms/step - loss: 0.1748 - accuracy: 0.9304 - val_loss: 0.4625 - val_accuracy: 0.8634\n",
            "Epoch 40/90\n",
            "250/250 [==============================] - 116s 465ms/step - loss: 0.1687 - accuracy: 0.9315 - val_loss: 0.5666 - val_accuracy: 0.8382\n",
            "Epoch 41/90\n",
            "250/250 [==============================] - 119s 475ms/step - loss: 0.1670 - accuracy: 0.9334 - val_loss: 0.4871 - val_accuracy: 0.8574\n",
            "Epoch 42/90\n",
            "250/250 [==============================] - 115s 460ms/step - loss: 0.1649 - accuracy: 0.9348 - val_loss: 0.4638 - val_accuracy: 0.8690\n",
            "Epoch 43/90\n",
            "250/250 [==============================] - 122s 488ms/step - loss: 0.1681 - accuracy: 0.9323 - val_loss: 0.4453 - val_accuracy: 0.8609\n",
            "Epoch 44/90\n",
            "250/250 [==============================] - 114s 454ms/step - loss: 0.1553 - accuracy: 0.9359 - val_loss: 0.4295 - val_accuracy: 0.8679\n",
            "Epoch 45/90\n",
            "250/250 [==============================] - 112s 446ms/step - loss: 0.1651 - accuracy: 0.9377 - val_loss: 0.4727 - val_accuracy: 0.8619\n",
            "Epoch 46/90\n",
            "250/250 [==============================] - 112s 449ms/step - loss: 0.1560 - accuracy: 0.9373 - val_loss: 0.5880 - val_accuracy: 0.8443\n",
            "Epoch 47/90\n",
            "250/250 [==============================] - 114s 457ms/step - loss: 0.1534 - accuracy: 0.9409 - val_loss: 0.4227 - val_accuracy: 0.8695\n",
            "Epoch 48/90\n",
            "250/250 [==============================] - 119s 475ms/step - loss: 0.1459 - accuracy: 0.9429 - val_loss: 0.4948 - val_accuracy: 0.8634\n",
            "Epoch 49/90\n",
            "250/250 [==============================] - 123s 492ms/step - loss: 0.1389 - accuracy: 0.9463 - val_loss: 0.5653 - val_accuracy: 0.8649\n",
            "Epoch 50/90\n",
            "250/250 [==============================] - 122s 488ms/step - loss: 0.1472 - accuracy: 0.9402 - val_loss: 0.5148 - val_accuracy: 0.8679\n",
            "Epoch 51/90\n",
            "250/250 [==============================] - 112s 449ms/step - loss: 0.1417 - accuracy: 0.9409 - val_loss: 0.5147 - val_accuracy: 0.8589\n",
            "Epoch 52/90\n",
            "250/250 [==============================] - 92s 369ms/step - loss: 0.1326 - accuracy: 0.9474 - val_loss: 0.5056 - val_accuracy: 0.8548\n",
            "Epoch 53/90\n",
            "250/250 [==============================] - 99s 394ms/step - loss: 0.1452 - accuracy: 0.9427 - val_loss: 0.4980 - val_accuracy: 0.8629\n",
            "Epoch 54/90\n",
            "250/250 [==============================] - 84s 334ms/step - loss: 0.1324 - accuracy: 0.9459 - val_loss: 0.5511 - val_accuracy: 0.8599\n",
            "Epoch 55/90\n",
            "250/250 [==============================] - 86s 343ms/step - loss: 0.1324 - accuracy: 0.9489 - val_loss: 0.5671 - val_accuracy: 0.8619\n",
            "Epoch 56/90\n",
            "250/250 [==============================] - 98s 390ms/step - loss: 0.1244 - accuracy: 0.9489 - val_loss: 0.5715 - val_accuracy: 0.8543\n",
            "Epoch 57/90\n",
            "250/250 [==============================] - 97s 387ms/step - loss: 0.1248 - accuracy: 0.9528 - val_loss: 0.5272 - val_accuracy: 0.8684\n",
            "Epoch 58/90\n",
            "250/250 [==============================] - 98s 393ms/step - loss: 0.1315 - accuracy: 0.9481 - val_loss: 0.5339 - val_accuracy: 0.8654\n",
            "Epoch 59/90\n",
            "250/250 [==============================] - 119s 477ms/step - loss: 0.1167 - accuracy: 0.9531 - val_loss: 0.5497 - val_accuracy: 0.8614\n",
            "Epoch 60/90\n",
            "250/250 [==============================] - 116s 464ms/step - loss: 0.1205 - accuracy: 0.9541 - val_loss: 0.5205 - val_accuracy: 0.8624\n",
            "Epoch 61/90\n",
            "250/250 [==============================] - 120s 478ms/step - loss: 0.1280 - accuracy: 0.9524 - val_loss: 0.5594 - val_accuracy: 0.8669\n",
            "Epoch 62/90\n",
            "250/250 [==============================] - 126s 506ms/step - loss: 0.1169 - accuracy: 0.9545 - val_loss: 0.5217 - val_accuracy: 0.8695\n",
            "Epoch 63/90\n",
            "250/250 [==============================] - 130s 518ms/step - loss: 0.1220 - accuracy: 0.9517 - val_loss: 0.5623 - val_accuracy: 0.8715\n",
            "Epoch 64/90\n",
            "250/250 [==============================] - 126s 503ms/step - loss: 0.1143 - accuracy: 0.9570 - val_loss: 0.6025 - val_accuracy: 0.8730\n",
            "Epoch 65/90\n",
            "250/250 [==============================] - 120s 480ms/step - loss: 0.1164 - accuracy: 0.9539 - val_loss: 0.5737 - val_accuracy: 0.8589\n",
            "Epoch 66/90\n",
            "250/250 [==============================] - 100s 400ms/step - loss: 0.1056 - accuracy: 0.9585 - val_loss: 0.5100 - val_accuracy: 0.8684\n",
            "Epoch 67/90\n",
            "250/250 [==============================] - 117s 466ms/step - loss: 0.1186 - accuracy: 0.9516 - val_loss: 0.5875 - val_accuracy: 0.8659\n",
            "Epoch 68/90\n",
            "250/250 [==============================] - 121s 485ms/step - loss: 0.1214 - accuracy: 0.9525 - val_loss: 0.5318 - val_accuracy: 0.8609\n",
            "Epoch 69/90\n",
            "250/250 [==============================] - 119s 474ms/step - loss: 0.1084 - accuracy: 0.9582 - val_loss: 0.6016 - val_accuracy: 0.8523\n",
            "Epoch 70/90\n",
            "250/250 [==============================] - 121s 485ms/step - loss: 0.1117 - accuracy: 0.9569 - val_loss: 0.5074 - val_accuracy: 0.8664\n",
            "Epoch 71/90\n",
            "250/250 [==============================] - 123s 491ms/step - loss: 0.1135 - accuracy: 0.9578 - val_loss: 0.6662 - val_accuracy: 0.8558\n",
            "Epoch 72/90\n",
            "250/250 [==============================] - 124s 495ms/step - loss: 0.1241 - accuracy: 0.9540 - val_loss: 0.5307 - val_accuracy: 0.8684\n",
            "Epoch 73/90\n",
            "250/250 [==============================] - 107s 427ms/step - loss: 0.1000 - accuracy: 0.9599 - val_loss: 0.5562 - val_accuracy: 0.8745\n",
            "Epoch 74/90\n",
            "250/250 [==============================] - 121s 484ms/step - loss: 0.1098 - accuracy: 0.9572 - val_loss: 0.5411 - val_accuracy: 0.8649\n",
            "Epoch 75/90\n",
            "250/250 [==============================] - 130s 518ms/step - loss: 0.1121 - accuracy: 0.9567 - val_loss: 0.6186 - val_accuracy: 0.8574\n",
            "Epoch 76/90\n",
            "250/250 [==============================] - 125s 498ms/step - loss: 0.1015 - accuracy: 0.9588 - val_loss: 0.6424 - val_accuracy: 0.8654\n",
            "Epoch 77/90\n",
            "250/250 [==============================] - 120s 478ms/step - loss: 0.1078 - accuracy: 0.9594 - val_loss: 0.5491 - val_accuracy: 0.8730\n",
            "Epoch 78/90\n",
            "250/250 [==============================] - 125s 501ms/step - loss: 0.1032 - accuracy: 0.9579 - val_loss: 0.6161 - val_accuracy: 0.8619\n",
            "Epoch 79/90\n",
            "250/250 [==============================] - 112s 446ms/step - loss: 0.1006 - accuracy: 0.9589 - val_loss: 0.5277 - val_accuracy: 0.8710\n",
            "Epoch 80/90\n",
            "250/250 [==============================] - 113s 452ms/step - loss: 0.0957 - accuracy: 0.9614 - val_loss: 0.6079 - val_accuracy: 0.8624\n",
            "Epoch 81/90\n",
            "250/250 [==============================] - 108s 432ms/step - loss: 0.0982 - accuracy: 0.9621 - val_loss: 0.6252 - val_accuracy: 0.8548\n",
            "Epoch 82/90\n",
            "250/250 [==============================] - 102s 406ms/step - loss: 0.0970 - accuracy: 0.9625 - val_loss: 0.6299 - val_accuracy: 0.8634\n",
            "Epoch 83/90\n",
            "250/250 [==============================] - 107s 428ms/step - loss: 0.1031 - accuracy: 0.9621 - val_loss: 0.7482 - val_accuracy: 0.8432\n",
            "Epoch 84/90\n",
            "250/250 [==============================] - 117s 469ms/step - loss: 0.0923 - accuracy: 0.9641 - val_loss: 0.6179 - val_accuracy: 0.8619\n",
            "Epoch 85/90\n",
            "250/250 [==============================] - 99s 396ms/step - loss: 0.0989 - accuracy: 0.9632 - val_loss: 0.5978 - val_accuracy: 0.8629\n",
            "Epoch 86/90\n",
            "250/250 [==============================] - 99s 394ms/step - loss: 0.0944 - accuracy: 0.9626 - val_loss: 0.6442 - val_accuracy: 0.8664\n",
            "Epoch 87/90\n",
            "250/250 [==============================] - 103s 413ms/step - loss: 0.1079 - accuracy: 0.9585 - val_loss: 0.5507 - val_accuracy: 0.8644\n",
            "Epoch 88/90\n",
            "250/250 [==============================] - 102s 408ms/step - loss: 0.0949 - accuracy: 0.9631 - val_loss: 0.6224 - val_accuracy: 0.8720\n",
            "Epoch 89/90\n",
            "250/250 [==============================] - 102s 409ms/step - loss: 0.0992 - accuracy: 0.9638 - val_loss: 0.6300 - val_accuracy: 0.8624\n",
            "Epoch 90/90\n",
            "250/250 [==============================] - 96s 385ms/step - loss: 0.0949 - accuracy: 0.9644 - val_loss: 0.6767 - val_accuracy: 0.8624\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x171fecf2370>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Vamos a utilizar image enhancement (o image data augmentation) para obtener un buen resultado en el conjunto de entrenamiento y testing y evitar el overfitting.\n",
        "# Se cogerá una selección aleatoria del lote de imágenes y se le harán transformaciones aleatorias. Se rotarán, se escalarán...\n",
        "# Con rescale 1/255 transformamos los píxeles para que vayan de 0 a 1.\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# target_size: tamaño al que se escalarán las imágenes.\n",
        "# batch_size: tamaño del lote de datos.\n",
        "# class_mode: tenemos dos clases, perros y gatos, por lo que este método de clasificación nos vale\n",
        "training_dataset = train_datagen.flow_from_directory('./dataset/training_set',\n",
        "                                                    target_size=(128, 128),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "testing_dataset = test_datagen.flow_from_directory('./dataset/test_set',\n",
        "                                                target_size=(128, 128),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n",
        "\n",
        "# steps_per_epoch: cuantos batches de muestras se ejecutan antes de finalizar el epoch. Colocamos el conjunto completo de imágenes que tenemos dividido por el tamaño de batch.\n",
        "# validation_steps: cuantos batches de muestras se ejecutan en la validación al final de cada epoch. Colocamos el conjunto completo de imágenes de test que tenemos dividido por el tamaño de batch.\n",
        "classifier.fit(training_dataset,\n",
        "                        steps_per_epoch=training_dataset.n//32,\n",
        "                        epochs=90,\n",
        "                        validation_data=testing_dataset,\n",
        "                        validation_steps=testing_dataset.n//32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KwHuYDWfcPEZ"
      },
      "source": [
        "# Parte 3 - Cómo hacer nuevas predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Z7k7KG9SZOjL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n",
            "{'cats': 0, 'dogs': 1}\n",
            "The image 1 is a dog\n",
            "The image 2 is a cat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.utils import load_img, img_to_array\n",
        "# leemos la imagen\n",
        "test_image = load_img('./dataset/single_prediction/cat_or_dog_1.jpg', target_size = (128, 128))\n",
        "# la convertimos a array\n",
        "test_image_ar = img_to_array(test_image)\n",
        "# expand_dims expande la forma del array. Inserta un nuevo eje que aparece en la posición especificada en axis, en la forma resultante del array expandido\n",
        "# Vamos a insertar una dimensión x. Es como decir que hacemos un array con un solo elemento que es es la imagen.\n",
        "# Para predecir, los modelos esperan un array\n",
        "# test_image_exp = np.expand_dims(test_image_ar, axis = 0)\n",
        "\n",
        "# en lugar de usar expand simplemente vamos a hacer un array con las dos imágenes y vamos a predecir ambas\n",
        "test_image2 = load_img('./dataset/single_prediction/cat_or_dog_2.jpg', target_size = (128, 128))\n",
        "test_image2 = img_to_array(test_image2)\n",
        "test_images = np.array([test_image_ar, test_image2])\n",
        "\n",
        "# realizamos la predicción\n",
        "result = classifier.predict(test_images)\n",
        "# Un 0 es gato, un 1 perro\n",
        "print(training_dataset.class_indices)\n",
        "\n",
        "# Imprimimos resultado\n",
        "for i in range(0, test_images.shape[0]):\n",
        "    # la guardamos en una variable\n",
        "    prediction = result[i][0]\n",
        "    if prediction == 0:\n",
        "        prediction_str = 'cat'\n",
        "    else:\n",
        "        prediction_str = 'dog'\n",
        "    # la imprimimos\n",
        "    print(f\"The image {i + 1} is a {prediction_str}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3cdeb3212b9cd6cb3adfc9aa28385dec25cb135a233d680843bd721e460bd758"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
